{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPEzLuIQMBE/rhn88CLGyFz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Naomie25/DI-Bootcamp/blob/main/Week8_Day1_ExerciceXP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Exercise 1: Traditional vs. Modern NLP: A Comparative Analysis"
      ],
      "metadata": {
        "id": "cM3QRrS13wAt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| **Aspect**                 | **Traditional NLP**                                                                              | **Modern NLP**                                                                                                       |\n",
        "| -------------------------- | ------------------------------------------------------------------------------------------------ | -------------------------------------------------------------------------------------------------------------------- |\n",
        "| **Feature Engineering**    | Manual feature engineering (e.g., TF-IDF, POS tags, dependency parsing)                          | Automatic feature extraction via deep learning models                                                                |\n",
        "| **Word Representations**   | Static embeddings (e.g., one-hot, Word2Vec, GloVe)                                               | Contextual embeddings (e.g., BERT, RoBERTa)                                                                          |\n",
        "| **Model Architectures**    | Shallow models (e.g., Naïve Bayes, SVM, Logistic Regression)                                     | Deep models (e.g., Transformers, LSTMs, GRUs)                                                                        |\n",
        "| **Training Methodology**   | Task-specific training from scratch                                                              | Pre-training on large corpora + fine-tuning on downstream tasks                                                      |\n",
        "| **Key Examples of Models** | Naïve Bayes, SVM, HMM, CRF                                                                       | BERT, GPT, RoBERTa, T5, LLaMA                                                                                        |\n",
        "| **Advantages**             | - Simple, interpretable<br>- Low resource requirements<br>- Fast to train                        | - State-of-the-art performance<br>- Handles complex language tasks<br>- Adaptable across tasks via transfer learning |\n",
        "| **Disadvantages**          | - Labor-intensive feature design<br>- Limited in capturing context<br>- Struggles with ambiguity | - High computational cost<br>- Requires large datasets<br>- Less interpretable                                       |\n"
      ],
      "metadata": {
        "id": "zg29vzqw37Cb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Impact of Evolution on Scalability and Efficiency\n",
        "\n",
        "\n",
        "1. Scalability:\n",
        "\n",
        "Traditional NLP: Scaling models to new languages or domains required rebuilding manual features and retraining from scratch, limiting scalability.\n",
        "\n",
        "Modern NLP: Pre-trained models (like BERT or GPT) serve as general-purpose language engines, enabling rapid adaptation to new tasks via fine-tuning, vastly improving scalability across tasks, languages, and industries.\n",
        "\n",
        "2. Efficiency:\n",
        "\n",
        "Traditional NLP: More efficient in low-data environments due to simpler models, but less efficient overall in handling complex tasks (e.g., semantic understanding).\n",
        "\n",
        "Modern NLP: Although computationally expensive during pre-training, modern models offer high efficiency in reuse across multiple tasks. Once pre-trained, these models avoid repeated ground-up training, streamlining development workflows.\n",
        "\n",
        "3. Practical Implications:\n",
        "\n",
        "APIs and cloud services now offer access to pre-trained models (e.g., OpenAI, Hugging Face), democratizing NLP.\n",
        "\n",
        "Modern NLP supports real-time applications (like chatbots and translation) that were impractical at scale with traditional methods.\n",
        "\n"
      ],
      "metadata": {
        "id": "wqj1sftY4FM8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise 2: LLM Architecture and Application Scenarios"
      ],
      "metadata": {
        "id": "KEhreSuv4ltX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| **Model** | **Core Architecture**                                                                                                                                                                                                                         | **Best Real-World Application**                                                | **Why This Model Excels for It**                                                                                                                             |\n",
        "| --------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------ |\n",
        "| **BERT**  | - **Bidirectional Transformer Encoder**<br>- **Masked Language Modeling (MLM)**: predicts missing words in a sentence.<br>- Learns context from both left and right simultaneously.                                                           | **Text Classification (e.g., sentiment analysis, spam detection)**             | BERT’s bidirectional attention captures full sentence context, improving understanding of text nuances necessary for classification tasks.                   |\n",
        "| **GPT**   | - **Unidirectional (left-to-right) Transformer Decoder**<br>- **Causal Language Modeling (CLM)**: predicts next word based only on previous tokens.<br>- Focused on sequential generation.                                                    | **Text Generation (e.g., chatbots, content creation, code generation)**        | GPT’s architecture is optimized for **generative tasks**, as it predicts text token by token, naturally producing coherent continuations of prompts.         |\n",
        "| **T5**    | - **Encoder-Decoder Transformer**<br>- **Text-to-Text Framework**: all tasks (classification, summarization, translation) are treated as converting input text into output text.<br>- Pre-trained using **Span Corruption** (similar to MLM). | **Multi-task Learning (e.g., summarization, translation, question answering)** | T5’s text-to-text approach makes it flexible for many NLP tasks, enabling a **single unified model** for diverse applications without architectural changes. |\n"
      ],
      "metadata": {
        "id": "8j-84hk74z-z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise 3: The Benefits and Ethical Considerations of Pre-training"
      ],
      "metadata": {
        "id": "7aUi7mpV4-Iu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### Five Main Benefits of Pre-Trained Models\n",
        "\n",
        "1. **Better at Handling New Things (Improved Generalization)**\n",
        "   Because pre-trained models learn from huge amounts of text, they become good at understanding many different situations, even ones they haven’t seen before.\n",
        "\n",
        "2. **Less Need for Labeled Data**\n",
        "   Normally, you need a lot of examples with labels (like “this is positive” or “this is spam”) to train a model. But pre-trained models already know a lot, so you only need **a small amount of labeled data** to teach them your task.\n",
        "\n",
        "3. **Faster Training (Fine-Tuning)**\n",
        "   Instead of starting from zero, you just adjust (fine-tune) a pre-trained model for your job. This is **quicker and cheaper** than training a model from scratch.\n",
        "\n",
        "4. **Transfer Learning**\n",
        "   Pre-trained models can **transfer what they know** to different topics. For example, a model trained on general English can quickly learn to understand medical or legal language.\n",
        "\n",
        "5. **More Reliable (Robustness)**\n",
        "   Since these models have seen so much text from so many places, they’re **better at handling weird sentences, spelling mistakes, or strange phrasing** without making mistakes.\n",
        "\n",
        "---\n",
        "\n",
        "###  Ethical Issues\n",
        "\n",
        "1. **Bias**\n",
        "   If the training data has unfair ideas (like stereotypes), the model can **repeat those unfair ideas** when it answers people.\n",
        "\n",
        "2. **False Information**\n",
        "   Because models learn from the internet, they might **repeat wrong or fake information**, without knowing it’s false.\n",
        "\n",
        "3. **Bad Uses (Misuse)**\n",
        "   People can use these models to create **fake news, scams, or harmful content** like fake messages pretending to be from real people.\n",
        "\n",
        "---\n",
        "\n",
        "###  How to Reduce These Problems\n",
        "\n",
        "1. **Fixing Bias**\n",
        "\n",
        "* Choose more balanced and fair training data.\n",
        "* Use special techniques to help the model avoid repeating stereotypes.\n",
        "* Test the model regularly to check if it’s being unfair.\n",
        "\n",
        "2. **Stopping False Information**\n",
        "\n",
        "* Remove bad-quality data before training.\n",
        "* Teach the model to recognize when it’s unsure, so it doesn’t give wrong answers confidently.\n",
        "* Add fact-checking steps during training or when the model gives answers.\n",
        "\n",
        "3. **Using Models Safely**\n",
        "\n",
        "* Give access to the model through **controlled systems (like APIs)**, so people can’t misuse it easily.\n",
        "* Add filters to block dangerous or harmful responses.\n",
        "* Be clear about how the model was trained and used, and make sure people follow rules when using it.\n"
      ],
      "metadata": {
        "id": "azgvbLZO4-hc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise 4 : Transformer Architecture Deep Dive"
      ],
      "metadata": {
        "id": "TMw89EDV5zsA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1- Explain Self-Attention and Multi-Head Attention:"
      ],
      "metadata": {
        "id": "L5GhIp5Z7GYY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Self-Attention**?\n",
        "\n",
        "In a sentence, **each word looks at the other words to understand their relationships**. This process is called **self-attention**. It helps the model understand which words are important to each other.\n",
        "\n",
        "###  How does Self-Attention work?\n",
        "\n",
        "1. **Every word gets a score showing how much attention it should pay to other words**.\n",
        "   Example: For the word “it”, the model checks every other word (like “dog”, “chased”, “cat”) and scores them to decide which one is most related.\n",
        "\n",
        "2. These scores are used to create a **weighted version of the sentence**, where important words have higher influence.\n",
        "\n",
        "3. Each word then uses this new information to build a better understanding of its meaning in the sentence.\n",
        "\n",
        "In short:\n",
        " **Each word learns by looking at all other words and deciding who matters most.**\n",
        "\n",
        "---\n",
        "\n",
        "## **Multi-Head Attention**\n",
        "\n",
        "Instead of doing self-attention just **once**, **multi-head attention** lets the model do it **several times, in different ways, at the same time**.\n",
        "\n",
        "Each \"head\" can focus on different things:\n",
        "\n",
        "* One head might focus on **grammar relationships** (like subjects and verbs).\n",
        "* Another head might focus on **semantic meaning** (who did what to whom).\n",
        "* Another head could focus on **long-distance relationships** in the sentence.\n",
        "\n",
        "**Why is this better?**\n",
        "Because different heads help the model learn from **different perspectives**, which makes its understanding much deeper and more accurate.\n",
        "\n",
        "---\n",
        "\n",
        "##  Why Multi-Head Attention is Better than Single-Head\n",
        "\n",
        "* **Single-head attention** can only focus on **one thing** at a time.\n",
        "* **Multi-head attention** lets the model **look at many aspects at once**, making it smarter and more flexible.\n",
        "\n",
        "---\n",
        "\n",
        "##  Summary\n",
        "\n",
        "* **Self-Attention** = Each word learns who matters in the sentence.\n",
        "* **Multi-Head Attention** = The model looks at the sentence from **many angles at once**, like having many mini-experts working together.\n",
        "\n"
      ],
      "metadata": {
        "id": "_ZyJC1Nz50u1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2- Pre-training Objectives:"
      ],
      "metadata": {
        "id": "0_qRWCrx7DW4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| **Aspect**         | **Masked Language Modeling (MLM)**                                                                                        | **Causal Language Modeling (CLM)**                                                   |\n",
        "| ------------------ | ------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------ |\n",
        "| **How it works**   | Some words in the sentence are **hidden (masked)**, and the model learns to guess them using both left and right context. | The model predicts the **next word**, using only the previous words (left-to-right). |\n",
        "| **Direction**      | **Bidirectional** (looks at the whole sentence at once).                                                                  | **Unidirectional** (looks left-to-right).                                            |\n",
        "| **Example Models** | BERT, RoBERTa                                                                                                             | GPT, GPT-2, GPT-3, GPT-4                                                             |\n",
        "| **Main Goal**      | Understand sentence meaning deeply.                                                                                       | Generate text in a logical, step-by-step flow.                                       |\n"
      ],
      "metadata": {
        "id": "18GHK1nZ6PsJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- MLM Scenario\n",
        "\n",
        "Task: Sentiment Analysis (understanding if a review is positive or negative).\n",
        "\n",
        "Why: MLM helps models understand full sentence meaning, which is important for analyzing tone or opinion.\n",
        "\n",
        "- CLM Scenario\n",
        "\n",
        "Task: Text Generation (like writing stories or chatbot replies).\n",
        "\n",
        "Why: CLM is perfect for generating the next word step-by-step, creating natural flowing text.\n",
        "\n"
      ],
      "metadata": {
        "id": "yXQfsaP87MmT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why Early BERT Used NSP (Next Sentence Prediction)\n",
        "\n",
        "What NSP Did:\n",
        "\n",
        "BERT was trained to guess if Sentence B follows Sentence A. This was meant to help BERT understand relationships between sentences.\n",
        "\n",
        "Why NSP Was Dropped in Modern Models:\n",
        "\n",
        "Later research showed NSP didn’t help much and sometimes hurt performance. Models like RoBERTa removed NSP and just focused on MLM, resulting in better understanding without NSP."
      ],
      "metadata": {
        "id": "qxg25T2g7AX1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " 3- Transformer Model Selection\n",
        "\n",
        " | **Task**                                                   | **Best Model Type**                       | **Why**                                                                                                                                    |\n",
        "| ---------------------------------------------------------- | ----------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------ |\n",
        "| **Analyze customer reviews (positive, negative, neutral)** | **Encoder-only** (like BERT)              | Encoder models are great at **understanding text** and producing a summary of meaning, which is perfect for classification tasks.          |\n",
        "| **Chatbot that generates creative responses**              | **Decoder-only** (like GPT)               | Decoder models are designed to **generate text**, predicting one word at a time in a sequence, ideal for conversations.                    |\n",
        "| **Translate documents (English to Spanish)**               | **Encoder-Decoder** (like T5 or MarianMT) | Encoder reads and understands the source sentence, Decoder generates the translated version. This combo is best for **translation tasks**. |\n"
      ],
      "metadata": {
        "id": "L_inQi1h7nJI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " 4- Positional Encoding\n",
        "\n",
        "- Why It’s Needed:\n",
        "Transformers process all words at the same time (parallel processing), unlike RNNs which process step-by-step. This means transformers don’t naturally know the order of words in a sentence.\n",
        "\n",
        "- Positional Encoding adds information about the position of each word (first, second, third...) so the model understands the sequence."
      ],
      "metadata": {
        "id": "1sapI3YK7qrR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example Problem Without Positional Encoding:\n",
        "\n",
        "Sentence:\n",
        "\n",
        "“The cat sat on the mat.”\n",
        "\n",
        "Without positional encoding, the model might think:\n",
        "\n",
        "“Mat sat the cat on the.”\n",
        "...is the same sentence!\n",
        "\n",
        "This is because, without knowing word positions, all words are just a bag of words to the model.\n",
        "\n",
        "So positional encoding tells the model:\n",
        "\n",
        "“The” comes first\n",
        "\n",
        "“cat” comes second\n",
        "\n",
        "“sat” comes third...\n",
        "Which helps the model understand sentence structure correctly."
      ],
      "metadata": {
        "id": "PbhmxxwJ77yd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise 5: BERT Variations - Choose Your Detective"
      ],
      "metadata": {
        "id": "5Oen0IkH8EZ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| **Scenario**                                                 | **Best BERT Variant** | **Why?**                                                                                                                                                        |\n",
        "| ------------------------------------------------------------ | --------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
        "| **1. Real-time sentiment analysis on mobile app**            | **DistilBERT**        | It's a **smaller, lighter version of BERT**, trained to be fast and efficient with **lower memory and power needs**—perfect for mobile or real-time use.        |\n",
        "| **2. Legal document research needing high accuracy**         | **RoBERTa**           | RoBERTa is trained on **larger datasets** and uses optimized training, leading to **higher accuracy** for complex, detail-heavy tasks like legal text analysis. |\n",
        "| **3. Global customer support in many languages**             | **XLM-RoBERTa**       | XLM-RoBERTa is trained on **100+ languages**, making it ideal for **multilingual applications** like global customer support.                                   |\n",
        "| **4. Efficient pretraining and token replacement detection** | **ELECTRA**           | ELECTRA uses a unique training method based on **replaced token detection**, making pretraining **faster and more efficient** than masked language modeling.    |\n",
        "| **5. Efficient NLP in resource-limited environments**        | **ALBERT**            | ALBERT reduces model size with **parameter sharing** and **factorized embeddings**, keeping accuracy while being more **memory-efficient**.                     |\n"
      ],
      "metadata": {
        "id": "l635qshy8OGo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| **Model**       | **Training Data & Method**                         | **Model Size & Efficiency**            | **Optimizations / Innovations**                | **Best Use Cases**                                      |\n",
        "| --------------- | -------------------------------------------------- | -------------------------------------- | ---------------------------------------------- | ------------------------------------------------------- |\n",
        "| **RoBERTa**     | More data than BERT, no NSP, dynamic masking       | Large; similar to or bigger than BERT  | Better masking, removes NSP, longer training   | High-accuracy tasks (legal, medical, research)          |\n",
        "| **ALBERT**      | Same as BERT, but with parameter sharing           | Much smaller than BERT, very efficient | Parameter sharing, factorized embeddings       | Resource-constrained but accuracy-focused tasks         |\n",
        "| **DistilBERT**  | Trained by distillation from BERT                  | 40% smaller, 60% faster                | Knowledge distillation (learns from full BERT) | Mobile apps, real-time tasks, fast predictions          |\n",
        "| **ELECTRA**     | Replaced token detection (more efficient than MLM) | Smaller/faster pretraining than BERT   | Generator-discriminator setup for pretraining  | Fast pretraining, efficient tasks, classification tasks |\n",
        "| **XLM-RoBERTa** | Huge multilingual datasets, no NSP                 | Large, similar to RoBERTa              | Supports 100+ languages, dynamic masking       | Multilingual tasks: translation, global chatbots        |\n"
      ],
      "metadata": {
        "id": "6A8gtYux8a3B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise 6: Softmax Temperature - The Randomness Regulator"
      ],
      "metadata": {
        "id": "j9IwuHGD8g-F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Temperature Scenarios\n",
        "\n",
        "- Temperature 0.2\n",
        "\n",
        "The model chooses the most likely words most of the time.\n",
        "\n",
        "Output is predictable, repetitive, and conservative.\n",
        "\n",
        "Example: \"The cat sat on the mat.\" (Simple, factual).\n",
        "\n",
        "- Temperature 1.0\n",
        "\n",
        "Balanced randomness and predictability.\n",
        "\n",
        "Output is coherent but not too rigid.\n",
        "\n",
        "The model can choose less common words if they make sense.\n",
        "\n",
        "Example: \"The curious cat settled comfortably on the warm mat.\"\n",
        "\n",
        "- Temperature 1.5\n",
        "\n",
        "The model chooses words more randomly.\n",
        "\n",
        "Output becomes more creative but riskier, possibly less coherent.\n",
        "\n",
        "Example: \"The whimsical feline sprawled atop the shimmering fabric near the fireplace.\""
      ],
      "metadata": {
        "id": "NLDFlLMv83S3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Application Design\n",
        "Personalized Bedtime Stories System\n",
        "Goal: Balance creativity with story coherence.\n",
        "\n",
        "Strategy:\n",
        "\n",
        "Use a higher temperature (e.g., 1.2 to 1.5) during storytelling parts to make the story more imaginative and varied.\n",
        "\n",
        "Lower the temperature (e.g., 0.7 to 1.0) for critical sections (e.g., moral of the story or plot conclusions) to keep the story logical.\n",
        "\n",
        "Result: Stories are fun and surprising, but don’t become confusing or chaotic."
      ],
      "metadata": {
        "id": "KBdY-t5f8_g9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Financial Reports Summarization System\n",
        "Goal: Ensure accuracy, consistency, and reliability.\n",
        "\n",
        "Strategy:\n",
        "\n",
        "Set temperature low (e.g., 0.2 to 0.5).\n",
        "\n",
        "This forces the model to stick to the most likely, factual language and avoid creative word choices that could mislead.\n",
        "\n",
        "Result: Summaries are clear, precise, and professional, minimizing the risk of errors.\n",
        "\n"
      ],
      "metadata": {
        "id": "L2I9bwv39H2T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Temperature and Bias\n",
        "How Temperature Affects Bias:\n",
        "\n",
        "Low Temperature:\n",
        "\n",
        "The model tends to repeat dominant patterns from training data.\n",
        "\n",
        "This can reinforce biases present in the data (since it favors \"safe\" or \"common\" outputs).\n",
        "\n",
        "High Temperature:\n",
        "\n",
        "The model explores less common word choices, which might reduce visible bias but also introduce randomness or even inappropriate content."
      ],
      "metadata": {
        "id": "4u-ss_kW9NpY"
      }
    }
  ]
}