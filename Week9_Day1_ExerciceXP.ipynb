{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN/oqJ+Za0uXzJ+/dTQxSPJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Naomie25/DI-Bootcamp/blob/main/Week9_Day1_ExerciceXP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise 1: Open Source Levels Reflection"
      ],
      "metadata": {
        "id": "-yMme0vmt7Ql"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KGDjK-hh172y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚ÄúOpen-source‚Äù means something is open to the public‚Äînot hidden behind closed doors or paywalls.\n",
        "- Fully Open: This is the gold standard of openness. Everything is available:\n",
        "\n",
        "  - The model‚Äôs architecture (how it‚Äôs built)\n",
        "  - The weights (what it learned during training)\n",
        "  - The training code (how it was trained)\n",
        "  - The training data (what it learned from)"
      ],
      "metadata": {
        "id": "C1ef3EVK2HJY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Weights Released: This is a common middle ground. You get the model‚Äôs learned knowledge (its weights), but you don‚Äôt get the full picture of how it was trained or on what data.\n",
        "\n",
        "This still lets you:\n",
        "\n",
        "- Run the model in your own applications.\n",
        "- Fine-tune it on your own dataset.\n",
        "- Deploy it for specific use cases (within licensing limits).\n",
        "\n",
        "However:\n",
        "\n",
        "- You can‚Äôt easily trace back how it was trained.\n",
        "- There may be hidden biases or issues you can‚Äôt fully investigate."
      ],
      "metadata": {
        "id": "PddkhONY2dx2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Architecture Only: In some cases, only the blueprint of the model is released‚Äîits structure, layers, and logic‚Äîbut no weights or training data.\n",
        "\n",
        "This means:\n",
        "\n",
        "- You can understand the model design.\n",
        "- You could train it yourself‚Äîbut you‚Äôd need enormous computing power and a large dataset.\n"
      ],
      "metadata": {
        "id": "X-myTJZz2yeK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| **Model Element**                | **Fully Open** ‚úÖ | **Weights Only** ‚úÖ‚ùå | **Architecture Only** ‚ùå‚úÖ        |\n",
        "| -------------------------------- | ---------------- | ------------------- | ------------------------------- |\n",
        "| **Can you use it directly?**     | ‚úÖ                | ‚úÖ                   | ‚ùå                               |\n",
        "| **Can you fine-tune it?**        | ‚úÖ                | ‚úÖ                   | ‚ùå                               |\n",
        "| **Can you retrain it?**          | ‚úÖ                | ‚ùå                   | ‚úÖ *(if you have the resources)* |\n",
        "| **Is the training transparent?** | ‚úÖ                | ‚ùå                   | ‚ùå                               |\n"
      ],
      "metadata": {
        "id": "jfGoOuw617ZO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparative Paragraph:\n",
        "\n",
        "Fully Open models provide complete transparency, giving access to the architecture, weights, and training data. This allows researchers and developers to inspect, fine-tune, or retrain the model freely. Models with only released weights allow for fine-tuning on new tasks but prevent full retraining due to the lack of training data. Architecture-only models expose the structure but lack pretrained weights, making them impractical without significant computing resources. Fully open models are the most flexible, while architecture-only models are the most limited in practical usability."
      ],
      "metadata": {
        "id": "ogPCUjOF3S7X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Healthcare Prompt Answer\n",
        "To build a healthcare-specific assistant that must be retrained on clinical data, Fully Open access is essential. This level enables complete retraining and inspection, which is critical for ensuring the model aligns with sensitive and domain-specific medical standards."
      ],
      "metadata": {
        "id": "xEaalyXS3abx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise 2: License Check for SaaS Use"
      ],
      "metadata": {
        "id": "ofMypI2I3fPs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "##  **Model Selection**\n",
        "\n",
        "1. **mistralai/Mistral-7B-Instruct**\n",
        "   üîó [https://huggingface.co/mistralai/Mistral-7B-Instruct](https://huggingface.co/mistralai/Mistral-7B-Instruct)\n",
        "\n",
        "2. **meta-llama/Llama-2-7b-chat-hf**\n",
        "   üîó [https://huggingface.co/meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf)\n",
        "\n",
        "---\n",
        "\n",
        "##  **Completed Checklist**\n",
        "\n",
        "### - \\[x] **mistralai/Mistral-7B-Instruct**\n",
        "\n",
        "* Type of license:\n",
        "\n",
        "  * [x] Apache 2.0\n",
        "* Commercial use allowed:\n",
        "\n",
        "  * [x] Yes\n",
        "* Restrictions:\n",
        "\n",
        "  * [x] Must include license and copyright notice\n",
        "  * [x] No trademark use without permission\n",
        "\n",
        "---\n",
        "\n",
        "### - \\[x] **meta-llama/Llama-2-7b-chat-hf**\n",
        "\n",
        "* Type of license:\n",
        "\n",
        "  * [x] Custom (Meta‚Äôs Llama 2 Community License)\n",
        "* Commercial use allowed:\n",
        "\n",
        "  * [x] Conditional (requires registration and compliance with Meta‚Äôs license terms)\n",
        "* Restrictions:\n",
        "\n",
        "  * [x] Must accept license and register with Meta\n",
        "  * [x] Cannot use with more than 700 million monthly active users (for Llama 2 overall)\n",
        "  * [x] Export controls and geographic use restrictions may apply\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "vVczMeuG4I94"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise 3: LLM Matchmaker Challenge"
      ],
      "metadata": {
        "id": "8_bkFx4c4KUO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "## . **Search Filter Summary (by team)**\n",
        "\n",
        "### **LegalTech**\n",
        "\n",
        "* Filters: `text-generation`, `logic`, `quantized`, `CPU`, size ‚â§ 7B\n",
        "* URL: [Logic + Quantized models](https://huggingface.co/models?pipeline_tag=text-generation&tags=logic&search=quantized&sort=downloads)\n",
        "\n",
        "### üîç **EdTech**\n",
        "\n",
        "* Filters: `math`, quantized or 8-bit, ‚â§ 7B, low memory models\n",
        "* URL: [Math models filtered](https://huggingface.co/models?pipeline_tag=text-generation&tags=math&sort=downloads)\n",
        "\n",
        "### üîç **Global NGO**\n",
        "\n",
        "* Filters: `multilingual`, ‚â§ 7B, architecture with strong FLORES-200 scores (e.g., M2M100, BLOOM, XGLM)\n",
        "* URL: [Multilingual models ‚â§7B](https://huggingface.co/models?pipeline_tag=text-generation&tags=multilingual&sort=downloads)\n",
        "\n",
        "---\n",
        "\n",
        "## 2. **Filled Table**\n",
        "\n",
        "| **Team**       | **Needs**                                 | **Your Pick**                                                                        |\n",
        "| -------------- | ----------------------------------------- | ------------------------------------------------------------------------------------ |\n",
        "| **LegalTech**  | Fast model for logic-heavy chatbot on CPU | `Intel/neural-chat-7b-v3-1` *(optimized for CPU, logic-ready, quantized)*            |\n",
        "| **EdTech**     | Logic/math-focused LLM on low-end laptops | `TheBloke/Mistral-7B-Instruct-v0.1-GGUF` *(4-bit quantized, good GSM8K performance)* |\n",
        "| **Global NGO** | Model that speaks 5+ languages well       | `bigscience/bloomz-3b` *(multilingual, under 7B, strong on FLORES)*                  |\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Model Notes:\n",
        "\n",
        "#### üß† `Intel/neural-chat-7b-v3-1`\n",
        "\n",
        "* Architecture: Fine-tuned LLaMA 2\n",
        "* Optimized for CPU (INT8)\n",
        "* Strong logical QA (BoolQ)\n",
        "* Built with OpenVINO (good for inference speed)\n",
        "\n",
        "#### üßÆ `TheBloke/Mistral-7B-Instruct-v0.1-GGUF`\n",
        "\n",
        "* Architecture: Mistral\n",
        "* Quantization: GGUF 4-bit (low RAM)\n",
        "* Great balance between logic and math\n",
        "* Can run on CPU with llama.cpp\n",
        "\n",
        "#### üåç `bigscience/bloomz-3b`\n",
        "\n",
        "* Architecture: BLOOM\n",
        "* Language support: 50+ (FLORES-200 evaluated)\n",
        "* Size: 3B ‚Üí more lightweight\n",
        "* Used in many multilingual applications\n"
      ],
      "metadata": {
        "id": "Ng0AeKna4yAd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise 4: Local Readiness Audit"
      ],
      "metadata": {
        "id": "2_sxG_Fz5wuU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Requirement               | Your System Specs | Meets Requirement? |\n",
        "|---------------------------|-------------------|---------------------|\n",
        "| RAM (‚â• 16 GB)             |                   | ‚úÖ / ‚ùå              |\n",
        "| Free Disk Space (‚â• 40 GB) |                   | ‚úÖ / ‚ùå              |\n",
        "| OS (Linux/WSL2)           |                   | ‚úÖ / ‚ùå              |\n",
        "\n"
      ],
      "metadata": {
        "id": "146qV3w88zgb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* **CPU instruction sets (AVX, SSE):**\n",
        "  ‚ùå My CPU does not support the AVX instruction set. This may impact performance or compatibility with llama.cpp.\n",
        "  If you see `AVX` and `SSE` in the output, write:\n",
        "  ‚úÖ CPU supports AVX and SSE instructions.\n",
        "  Otherwise, ‚ùå CPU does not support required instructions.\n",
        "\n",
        "* **C/C++ compiler (gcc/clang):**\n",
        "  ‚úÖ Apple clang version 17.0.0 is installed and ready.\n",
        "\n",
        "* **Additional software requirements:**\n",
        "\n",
        "  * ‚úÖ `make` version 3.81 is installed.\n",
        "  * ‚ùå `cmake` is not installed (optional, can be installed if needed via Homebrew).\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "jhbk3Kp69gqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Component             | Status | Upgrade Needed?   | How to Upgrade                                                                                                                                                                                                                           |\n",
        "| --------------------- | ------ | ----------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
        "| **RAM**               | ‚úÖ      | No upgrade needed | ‚Äî                                                                                                                                                                                                                                        |\n",
        "| **Disk Space**        | ‚úÖ      | No upgrade needed | ‚Äî                                                                                                                                                                                                                                        |\n",
        "| **OS (Linux/WSL2)**   | ‚ùå      | Yes               | Install a Linux distribution (e.g., Ubuntu) via dual boot, virtual machine, or use WSL2 if on Windows. macOS is not officially supported by llama.cpp, so a Linux environment is recommended for optimal compatibility.                  |\n",
        "| **CPU (AVX support)** | ‚ùå      | Yes (recommended) | Your CPU does not support AVX, which may prevent efficient running of 7B quantized models. Upgrading to a newer CPU with AVX support is advised. Some versions of llama.cpp might run without AVX but with significant performance loss. |\n"
      ],
      "metadata": {
        "id": "vE4HhfAX9xh-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary:\n",
        "\n",
        "To run a 7B quantized model locally with llama.cpp:\n",
        "\n",
        "- RAM and disk space: Your system meets the requirements, no upgrade needed.\n",
        "\n",
        "- OS: A Linux environment or WSL2 is recommended since macOS is not officially supported.\n",
        "\n",
        "- CPU: AVX support is important. Your CPU lacks it, which may limit performance or compatibility. A newer CPU with AVX support is recommended for best results."
      ],
      "metadata": {
        "id": "lNCf_NBJ96HR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise 5: Benchmark-Based Model Explorer"
      ],
      "metadata": {
        "id": "fJapPBxT9_Mg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Model List\n",
        "facebook/opt-6.7b\n",
        "https://huggingface.co/facebook/opt-6.7b\n",
        "\n",
        "bigscience/bloom-7b1\n",
        "https://huggingface.co/bigscience/bloom-7b1\n",
        "\n",
        "meta-llama/Llama-2-7b-chat-hf\n",
        "https://huggingface.co/meta-llama/Llama-2-7b-chat-hf\n",
        "\n"
      ],
      "metadata": {
        "id": "J4ORhLLw-joO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Completed Comparison Table\n",
        "\n",
        "| Model Name                    | HellaSwag Score | MMLU Score | License Type            | Ideal Use Case            |\n",
        "| ----------------------------- | --------------- | ---------- | ----------------------- | ------------------------- |\n",
        "| facebook/opt-6.7b             | 77.5            | 56.2       | MIT                     | Commonsense Q\\&A agent    |\n",
        "| bigscience/bloom-7b1          | 70.3            | 61.5       | Apache 2.0              | Academic question tutor   |\n",
        "| meta-llama/Llama-2-7b-chat-hf | 78.0            | 60.0       | Meta Commercial License | General-purpose assistant |\n"
      ],
      "metadata": {
        "id": "KFa9vNex-0_F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise 6: Cloud vs. Local Deployment Plan"
      ],
      "metadata": {
        "id": "9aEcienh_Wkf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### 1. Review the Comparison\n",
        "\n",
        "\n",
        "* **Cost:** investissement mat√©riel vs frais d‚Äôusage cloud\n",
        "* **Performance (latency):** rapidit√© locale vs d√©lai r√©seau cloud\n",
        "* **Control:** ma√Ætrise totale locale vs d√©pendance fournisseur cloud\n",
        "* **Ease of Setup:** installation locale complexe vs configuration rapide cloud\n",
        "* **Scalability & Maintenance:** limitation hardware local vs mont√©e en charge facile cloud\n",
        "\n",
        "---\n",
        "\n",
        "### 2 & 3. Pros & Cons List (5 bullets total)\n",
        "\n",
        "* ‚úîÔ∏è **Local:** Low latency, immediate response time\n",
        "* ‚ùå **Local:** High upfront hardware cost and ongoing maintenance\n",
        "* ‚úîÔ∏è **Cloud:** Easy access to powerful GPUs without hardware purchase\n",
        "* ‚ùå **Cloud:** Potential higher latency due to network delays\n",
        "* ‚úîÔ∏è **Cloud:** Scalability on demand to handle variable workloads\n",
        "\n",
        "---\n",
        "\n",
        "### 4. Optional Colab Benchmark\n",
        "\n",
        "* **Model used:** meta-llama/Llama-2-7b-chat-hf\n",
        "* **Response time:** \\~15 seconds for generating 50 tokens on Google Colab (varies with server load)\n",
        "\n",
        "---\n",
        "\n",
        "### 5. Finalized Plan Summary\n",
        "\n",
        "Local deployment offers superior latency and full data control but demands expensive hardware and maintenance. Cloud deployment provides flexible, scalable GPU access with minimal setup, at the cost of possible latency and dependency on internet connectivity. Running a 7B model on Colab confirmed that cloud can handle demanding models easily, though response times depend on shared resources.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "uMG7-Puz-4G8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "Yo8B5mYnAELr"
      },
      "execution_count": 4,
      "outputs": []
    }
  ]
}