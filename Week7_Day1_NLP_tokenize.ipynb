{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Naomie25/DI-Bootcamp/blob/main/Week7_Day1_NLP_tokenize.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NLP DAY I: INTRO"
      ],
      "metadata": {
        "id": "VyGObZqWb2Bs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "k3m5eq8bL3FU"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "import spacy\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install spaCy (if not already installed)\n",
        "!pip install -U spacy\n",
        "\n",
        "# Download the large English model\n",
        "!python -m spacy download en_core_web_sm\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3207giWMgkh",
        "outputId": "6761fdae-20bf-4da2-e1b6-a48ff6d8a4bb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.7)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.16.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.14.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.6.15)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m100.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en_core_web_sm')\n",
        "# RUN THIS ON THE TERMINAL:  python -m spacy download en_core_web_lg\n",
        "\n",
        "\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download the 'punkt_tab' resource\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fapbrDRMMB7M",
        "outputId": "67bb2fda-df96-40eb-a75a-23df81227679"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TOKENIZE TEXT\n"
      ],
      "metadata": {
        "id": "h0oyCh1GNJjk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'We are learning NLP. I am excited about it.'\n",
        "tokens = word_tokenize(text)\n",
        "print(tokens)\n",
        "\n",
        "span = tokens[1:7]\n",
        "print(span)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JtnohJZsNbdl",
        "outputId": "164842b3-ceb5-44d3-aace-3eb21b662186"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['We', 'are', 'learning', 'NLP', '.', 'I', 'am', 'excited', 'about', 'it', '.']\n",
            "['are', 'learning', 'NLP', '.', 'I', 'am']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "exercise 1 - lesson\n"
      ],
      "metadata": {
        "id": "acHZdGFmOdJ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = \"Why, sometimes I've believed as many as six impossible things before breakfast? There goes the shawl again!\"\n",
        "\n",
        "tokens = word_tokenize(doc)\n",
        "print(tokens)\n",
        "\n",
        "sentences = sent_tokenize(doc)\n",
        "print(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IpP9cX1tNfht",
        "outputId": "2ea8097c-a47d-4166-9d10-f3cd38d3a1ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Why', ',', 'sometimes', 'I', \"'ve\", 'believed', 'as', 'many', 'as', 'six', 'impossible', 'things', 'before', 'breakfast', '?', 'There', 'goes', 'the', 'shawl', 'again', '!']\n",
            "[\"Why, sometimes I've believed as many as six impossible things before breakfast?\", 'There goes the shawl again!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "eOh4iUt_Oqwo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stopwords Removal"
      ],
      "metadata": {
        "id": "RC_cfAyUOrt0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords # import the stopwords module\n",
        "\n",
        "print(stopwords.words('english'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eec3sLyeOsod",
        "outputId": "620ed980-6f99-44db-d70f-8720941fd7ad"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", 'her', 'here', 'hers', 'herself', \"he's\", 'him', 'himself', 'his', 'how', 'i', \"i'd\", 'if', \"i'll\", \"i'm\", 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', \"i've\", 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', 'shouldn', \"shouldn't\", \"should've\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", 'were', 'weren', \"weren't\", \"we've\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\"]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "exercise 2 - lesson"
      ],
      "metadata": {
        "id": "NGDF-VMAO-DE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = \"Linguistics and Natural Language Processing (NLP) are closely linked. \\n Linguistics is the scientific study of language, encompassing its structure, meaning, and context. \\n It provides foundational knowledge about language syntax, semantics, pragmatics, and phonetics. \\n NLP applies this linguistic knowledge in computational algorithms to enable computers to understand, interpret, and generate human language. By leveraging linguistic principles, NLP seeks to bridge the gap between human communication and computer understanding, enabling tasks like translation, sentiment analysis, and voice recognition.\"\n",
        "\n",
        "tokens = word_tokenize(doc)\n",
        "\n",
        "filtered_words = [word for word in tokens if not word.lower() in stopwords.words('english')]\n",
        "print(\"Original Tokens:\", tokens)\n",
        "print(\"Filtered Tokens:\", filtered_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vsr5E1C4O-1v",
        "outputId": "7eaea953-d8a7-4c5d-a94d-c21c41a86542"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Tokens: ['Linguistics', 'and', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', 'are', 'closely', 'linked', '.', 'Linguistics', 'is', 'the', 'scientific', 'study', 'of', 'language', ',', 'encompassing', 'its', 'structure', ',', 'meaning', ',', 'and', 'context', '.', 'It', 'provides', 'foundational', 'knowledge', 'about', 'language', 'syntax', ',', 'semantics', ',', 'pragmatics', ',', 'and', 'phonetics', '.', 'NLP', 'applies', 'this', 'linguistic', 'knowledge', 'in', 'computational', 'algorithms', 'to', 'enable', 'computers', 'to', 'understand', ',', 'interpret', ',', 'and', 'generate', 'human', 'language', '.', 'By', 'leveraging', 'linguistic', 'principles', ',', 'NLP', 'seeks', 'to', 'bridge', 'the', 'gap', 'between', 'human', 'communication', 'and', 'computer', 'understanding', ',', 'enabling', 'tasks', 'like', 'translation', ',', 'sentiment', 'analysis', ',', 'and', 'voice', 'recognition', '.']\n",
            "Filtered Tokens: ['Linguistics', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', 'closely', 'linked', '.', 'Linguistics', 'scientific', 'study', 'language', ',', 'encompassing', 'structure', ',', 'meaning', ',', 'context', '.', 'provides', 'foundational', 'knowledge', 'language', 'syntax', ',', 'semantics', ',', 'pragmatics', ',', 'phonetics', '.', 'NLP', 'applies', 'linguistic', 'knowledge', 'computational', 'algorithms', 'enable', 'computers', 'understand', ',', 'interpret', ',', 'generate', 'human', 'language', '.', 'leveraging', 'linguistic', 'principles', ',', 'NLP', 'seeks', 'bridge', 'gap', 'human', 'communication', 'computer', 'understanding', ',', 'enabling', 'tasks', 'like', 'translation', ',', 'sentiment', 'analysis', ',', 'voice', 'recognition', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEMMING\n"
      ],
      "metadata": {
        "id": "x7UC3Fo9PUDj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "stemmed = [stemmer.stem(word) for word in tokens]\n",
        "print(stemmed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kL5f9kGiPVmj",
        "outputId": "6a5d0aa5-b5cc-46d5-df30-e806121c5a6d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['we', 'are', 'learn', 'nlp', '.', 'i', 'am', 'excit', 'about', 'it', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LEMMATIZATION"
      ],
      "metadata": {
        "id": "KAMoe9AYPdAd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "#load spaCy's English language model\n",
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "id": "H8ans9ySPixX"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'We are learning NLP. I am excited about it.'\n",
        "\n",
        "# process the text using spaCy\n",
        "doc_spacy = nlp(text)\n",
        "\n",
        "filtered_doc = [word for word in doc_spacy if word not in stopwords.words('english')]\n",
        "\n",
        "lemmatized = [token.lemma_ for token in filtered_doc] #In spaCy the token is an obj and it has the attribute lemma_\n",
        "print('Lemmatized: ', lemmatized)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5KGWDSJOPxQB",
        "outputId": "b5dd45de-7a3f-42d3-90ef-31d22ed65dac"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatized:  ['we', 'be', 'learn', 'NLP', '.', 'I', 'be', 'excited', 'about', 'it', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stemming vs Lemmatization"
      ],
      "metadata": {
        "id": "7jmGDk79QziG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = '''Charles Lutwidge Dodgson, better known by his pen name Lewis Carroll, was an English author, poet, mathematician and photographer. His most notable works are Alice's Adventures in Wonderland (1865) and its sequel Through the Looking-Glass (1871).'''\n",
        "\n",
        "# Tokenize the document\n",
        "tokens = word_tokenize(doc)\n",
        "\n",
        "# Remove stop words and non-words for stemming\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_tokens_stemming = [word for word in tokens if word.isalnum() and word not in stop_words]\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "stemmed = [stemmer.stem(word) for word in filtered_tokens_stemming] # Use filtered_tokens_stemming here\n",
        "print('Stemmed:' ,stemmed)\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "#process the text using spaCy\n",
        "doc = nlp(doc)\n",
        "\n",
        "# Remove stop words for lemmatization\n",
        "filtered_doc_lemm = [token.lemma_ for token in doc if token.text.lower() not in stop_words and token.is_alpha] # Define and assign filtered_doc_lemm\n",
        "\n",
        "print('Lemmatized: ', filtered_doc_lemm) # Print the lemmatized output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LeCVVsuPQ6HC",
        "outputId": "8c38125f-4440-4f1f-9736-c594139f700c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemmed: ['charl', 'lutwidg', 'dodgson', 'better', 'known', 'pen', 'name', 'lewi', 'carrol', 'english', 'author', 'poet', 'mathematician', 'photograph', 'hi', 'notabl', 'work', 'alic', 'adventur', 'wonderland', '1865', 'sequel', 'through', '1871']\n",
            "Lemmatized:  ['Charles', 'Lutwidge', 'Dodgson', 'well', 'know', 'pen', 'name', 'Lewis', 'Carroll', 'english', 'author', 'poet', 'mathematician', 'photographer', 'notable', 'work', 'Alice', 'Adventures', 'Wonderland', 'sequel', 'Looking', 'Glass']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "POS (Part Of Speech)"
      ],
      "metadata": {
        "id": "lyQ50uXKRYEl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import pos_tag\n",
        "\n",
        "# Download the 'averaged_perceptron_tagger_eng' resource\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "tagged = pos_tag(tokens)\n",
        "for word, tag in tagged:\n",
        "    print(f'Word: {word}, POS: {tag}')\n",
        "\n",
        "\n",
        "#You can use the following code to check what each tag means:\n",
        "\n",
        "# Download the 'tagsets_json' resource, which contains tagsets_json and provides the data for nltk.help.upenn_tagset()\n",
        "nltk.download('tagsets_json') # Download the necessary JSON data for the tagsets\n",
        "\n",
        "nltk.help.upenn_tagset('NN')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kz1kN8KxRaB0",
        "outputId": "837f55bf-9be7-4430-e321-966dbcd013c3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word: Charles, POS: NNP\n",
            "Word: Lutwidge, POS: NNP\n",
            "Word: Dodgson, POS: NNP\n",
            "Word: ,, POS: ,\n",
            "Word: better, POS: JJR\n",
            "Word: known, POS: VBN\n",
            "Word: by, POS: IN\n",
            "Word: his, POS: PRP$\n",
            "Word: pen, POS: JJ\n",
            "Word: name, POS: NN\n",
            "Word: Lewis, POS: NNP\n",
            "Word: Carroll, POS: NNP\n",
            "Word: ,, POS: ,\n",
            "Word: was, POS: VBD\n",
            "Word: an, POS: DT\n",
            "Word: English, POS: JJ\n",
            "Word: author, POS: NN\n",
            "Word: ,, POS: ,\n",
            "Word: poet, POS: NN\n",
            "Word: ,, POS: ,\n",
            "Word: mathematician, POS: JJ\n",
            "Word: and, POS: CC\n",
            "Word: photographer, POS: NN\n",
            "Word: ., POS: .\n",
            "Word: His, POS: PRP$\n",
            "Word: most, POS: RBS\n",
            "Word: notable, POS: JJ\n",
            "Word: works, POS: NNS\n",
            "Word: are, POS: VBP\n",
            "Word: Alice, POS: NNP\n",
            "Word: 's, POS: POS\n",
            "Word: Adventures, POS: NNS\n",
            "Word: in, POS: IN\n",
            "Word: Wonderland, POS: NNP\n",
            "Word: (, POS: (\n",
            "Word: 1865, POS: CD\n",
            "Word: ), POS: )\n",
            "Word: and, POS: CC\n",
            "Word: its, POS: PRP$\n",
            "Word: sequel, POS: NN\n",
            "Word: Through, POS: IN\n",
            "Word: the, POS: DT\n",
            "Word: Looking-Glass, POS: NNP\n",
            "Word: (, POS: (\n",
            "Word: 1871, POS: CD\n",
            "Word: ), POS: )\n",
            "Word: ., POS: .\n",
            "NN: noun, common, singular or mass\n",
            "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
            "    investment slide humour falloff slick wind hyena override subhumanity\n",
            "    machinist ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package tagsets_json to /root/nltk_data...\n",
            "[nltk_data]   Unzipping help/tagsets_json.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Named Entity Recognition (NER)"
      ],
      "metadata": {
        "id": "bJ5cVo9rRjLv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import pos_tag, ne_chunk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download necessary NLTK resources, including 'maxent_ne_chunker_tab'\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')  # This downloads 'maxent_ne_chunker', including the needed tab file\n",
        "nltk.download('words')\n",
        "nltk.download('punkt')\n",
        "nltk.download('maxent_ne_chunker_tab') # Download the missing 'maxent_ne_chunker_tab' data\n",
        "\n",
        "doc = 'Apple is looking at buying U.K. startup for $1 billion'\n",
        "# Tokenize the doc using word_tokenize before passing to pos_tag\n",
        "tokens = word_tokenize(doc)\n",
        "tagged = pos_tag(tokens)\n",
        "\n",
        "entities = ne_chunk(tagged)\n",
        "print(entities)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cgZMsf4QRkAf",
        "outputId": "a4475c45-f6f6-4d4a-efd7-431adf250ab4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (GPE Apple/NNP)\n",
            "  is/VBZ\n",
            "  looking/VBG\n",
            "  at/IN\n",
            "  buying/VBG\n",
            "  U.K./NNP\n",
            "  startup/NN\n",
            "  for/IN\n",
            "  $/$\n",
            "  1/CD\n",
            "  billion/CD)\n"
          ]
        }
      ]
    }
  ]
}