{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Naomie25/DI-Bootcamp/blob/main/Week9_Day2_lesson2_tuto_langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building AI Chat Agents with LangChain, Mistral & Tavily"
      ],
      "metadata": {
        "id": "6GY9MpaKbzeX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overview\n",
        "\n",
        "In this tutorial, you will build a tool-enabled conversational agent using LangChain and the Mistral LLM. By combining the ReAct reasoning framework with custom tools—web search and a calculator—you’ll create an agent that can:\n",
        "\n",
        "* Dynamically call external services as needed.\n",
        "* Reason through tasks step-by-step.\n",
        "* Stream responses for a responsive user experience.\n",
        "\n",
        "### Tutorial Steps\n",
        "\n",
        "1. **Prerequisites**: Set up Python, a virtual environment, and account access.\n",
        "2. **Install Required Packages**: Add LangChain, community extensions, LangGraph, and Mistral bindings.\n",
        "3. **Configure Environment Variables**: Securely load API keys for LangSmith, Tavily, and Mistral.\n",
        "4. **Build a Search Tool**: Wrap the Tavily Search API as a LangChain Tool.\n",
        "5. **Define a Calculator Tool**: Create a safe arithmetic evaluator available to the agent.\n",
        "6. **Initialize the Mistral Chat Model**: Connect to Mistral via LangChain’s helper.\n",
        "7. **Build a ReAct Agent**: Enable the agent to interleave thought and action cycles.\n",
        "8. **Invoke the Agent**: Send queries and observe the reasoning trace and final answer.\n",
        "9. **High-Level API with LangGraph**: Use a simplified executor interface.\n",
        "10. **Streamed Outputs**: Display partial responses in real time.\n",
        "\n",
        "Each step explains both the code and the underlying purpose, guiding you from setup to a fully functional, interactive AI agent.\n"
      ],
      "metadata": {
        "id": "MJq58njOb1la"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Prerequisites\n",
        "\n",
        "Before you begin, ensure you have:\n",
        "\n",
        "* **Python 3.8+** installed.\n",
        "* A virtual environment (venv, conda, etc.) for isolating dependencies.\n",
        "* API keys for:\n",
        "\n",
        "  * **LangSmith** (tracing/debugging)\n",
        "  * **Tavily Search**\n",
        "  * **Mistral AI** (Mistral endpoint)\n",
        "* (Optional) A Jupyter or Colab environment."
      ],
      "metadata": {
        "id": "5vqAfHxjb5v2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Install Required Packages"
      ],
      "metadata": {
        "id": "BvwUSZI4cEeT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1wvuacItpQzl",
        "outputId": "76e3123e-a124-419e-c95b-958f865ccdcd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m52.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.9/143.9 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.1/293.1 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.8/442.8 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.3/50.3 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.6/151.6 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.5/216.5 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Core LangChain + community extensions + Mistral\n",
        "%pip install --quiet -U langchain langchain-community langchain-anthropic \\\n",
        "               langgraph langgraph-checkpoint-sqlite tavily-python \\\n",
        "               \"langchain[mistralai]\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **`pip install`**: The Python package installer.\n",
        "* **`--quiet`**: Reduces the amount of output shown (useful for cleaner logs).\n",
        "* **`-U`** (or `--upgrade`): Ensures you get the latest version of each package.\n",
        "\n",
        "The backslashes (`\\`) let you split the command over multiple lines for readability. The packages are:\n",
        "\n",
        "- 1. **`langchain`** : The core LangChain framework for building LLM applications (chains, agents, prompt templates, etc.).\n",
        "- 2. **`langchain-community`** : Community-contributed extensions and integrations (additional tools, helpers, etc.).\n",
        "- 3. **`langchain-anthropic`** : Support for Anthropic models if you plan to use them alongside Mistral.\n",
        "- 4. **`langgraph`** : A higher-level orchestration layer built on top of LangChain, useful for more complex pipelines.\n",
        "- 5. **`langgraph-checkpoint-sqlite`** : A plugin for `langgraph` that enables checkpointing state in a SQLite database, so you can resume or debug runs.\n",
        "- 6. **`tavily-python`** : The Python SDK for the Tavily Search API, which we will use to add web-search capabilities to our agent.\n",
        "- 7. **`\"langchain[mistralai]\"`** : An “extra” in pip syntax. This installs additional dependencies needed to connect to Mistral endpoints (e.g., HTTP client, authentication helpers).\n",
        "\n",
        "After running this command, you’ll have all the libraries installed and ready to import in your Python scripts or notebooks."
      ],
      "metadata": {
        "id": "I77DYiJPcJJA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Configure Environment Variables\n",
        "\n",
        "Before your code can call external services, you need to provide your API keys securely. We do this using environment variables and the `getpass` module to prompt without echoing your keys on screen.\n"
      ],
      "metadata": {
        "id": "QqPDufcXcMP-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What this does:\n",
        "\n",
        "* **`os.environ[...] = ...`** sets an environment variable in your Python session.\n",
        "* **`getpass.getpass(...)`** securely prompts you to type the key without showing it in the console.\n",
        "* **`LANGSMITH_TRACING`** toggles verbose tracing in LangSmith’s dashboard.\n",
        "* Checking `if not os.environ.get(...)` lets you pre‑set keys in your shell (`export KEY=...`) or a `.env` file if you prefer.\n",
        "\n",
        "### Obtaining API Keys\n",
        "To use these services, sign up and generate keys on their respective websites:\n",
        "* **LangSmith**:\n",
        "  1. Visit **[https://smith.langchain.com](https://smith.langchain.com)**\n",
        "  2. Sign in or create an account.\n",
        "  3. Navigate to **Account > API Keys**.\n",
        "  4. Click **New Key**, give it a name, and copy the generated value.\n",
        "  5. In your Python session or shell, set:\n",
        "\n",
        "     ```bash\n",
        "     export LANGSMITH_API_KEY=\"<your-langsmith-key>\"\n",
        "     export LANGSMITH_TRACING=\"true\"\n",
        "     ```\n",
        "\n",
        "* **Tavily Search**:\n",
        "    1. Go to **[https://developer.tavily.com](https://developer.tavily.com)** (or the Tavily website).\n",
        "    2. Sign up for a developer account.\n",
        "    3. In the dashboard, find **API Keys** and click **Create Key**.\n",
        "    4. Copy the key and set:\n",
        "\n",
        "     ```bash\n",
        "     export TAVILY_API_KEY=\"<your-tavily-key>\"\n",
        "     ```\n",
        "\n",
        "* **Mistral AI**:\n",
        "  1. Navigate to **[https://platform.mistral.ai](https://platform.mistral.ai)**.\n",
        "  2. Register or log in to your account.\n",
        "  3. Visit **API Keys** and select **Create Key**.\n",
        "  4. Copy the key and set:\n",
        "\n",
        "     ```bash\n",
        "     export MISTRAL_API_KEY=\"<your-mistral-key>\"\n",
        "     ```\n",
        "\n",
        "> **Tip:** You can store these in a `.env` file and load them in Python with `python-dotenv` for convenience.\n"
      ],
      "metadata": {
        "id": "ssSTs5mYcWIN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, getpass\n",
        "\n",
        "\n",
        "os.environ[\"LANGSMITH_API_KEY\"] =\"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\"\n",
        "os.environ[\"TAVILY_API_KEY\"]  = \"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\"\n",
        "os.environ[\"MISTRAL_API_KEY\"] = \"C69TRtdJxynci7Bf0RVUENq2vitNr81n\"\n",
        "# LangSmith (for tracing/debugging your agent’s steps)\n",
        "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
        "os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass(\"LangSmith API key: \")\n",
        "\n",
        "# Tavily Search\n",
        "os.environ[\"TAVILY_API_KEY\"] = getpass.getpass(\"Tavily API key: \")\n",
        "\n",
        "# Mistral\n",
        "if not os.environ.get(\"MISTRAL_API_KEY\"):\n",
        "    os.environ[\"MISTRAL_API_KEY\"] = getpass.getpass(\"Mistral API key: \")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D55ZAA4PpTBG",
        "outputId": "0ff85e99-2b5f-4da7-a6e3-1f59e7c230eb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LangSmith API key: ··········\n",
            "Tavily API key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Build a Search Tool\n",
        "\n",
        "LangChain’s **`Tool`** abstraction lets your agent call out to external capabilities (search, databases, calculators, etc.) as part of its thought–action cycle. Behind the scenes, each tool is just a Python function or class with a `run` method, plus a human-readable description.\n"
      ],
      "metadata": {
        "id": "MNpdzcdBcYoH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.tools.tavily_search import TavilySearchResults #This class is provided by the `langchain-community` package as a ready-made wrapper over the Tavily Search API.\n",
        "\n",
        "# Return up to 3 results when invoked\n",
        "search_tool = TavilySearchResults(max_results=3)\n",
        "\n",
        "# You can add more tools (calculator, SQL, custom API wrappers…) to this list.\n",
        "tools = [search_tool]\n"
      ],
      "metadata": {
        "id": "qdkTwZMKpXyY"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **`max_results=3`** tells the tool to fetch at most three search hits for each query.\n",
        "* Internally, calling `search_tool.run(query_string)` will:\n",
        "\n",
        "  1. Send your query to Tavily’s API\n",
        "  2. Receive a JSON payload of results (title, URL, snippet)\n",
        "  3. Return a formatted list (or string) your agent can parse\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "8Qrvb1f0cj-K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "model = init_chat_model(\n",
        "    \"mistral-instruct-v0.3\",        # lighter, more available model\n",
        "    model_provider=\"mistralai\",\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "3LvuCM6DpbmG"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_with_tools = model.bind_tools(tools)\n"
      ],
      "metadata": {
        "id": "YDlDB3fiqv8i"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When your agent processes a user question, it may decide: “I need more information, so I’ll call the **search\\_tool**.” Under the hood, LangChain:\n",
        "\n",
        "1. Inserts an action step into its reasoning chain: `Action: search_tool.run`\n",
        "2. Passes the user’s query into `search_tool.run(query)`\n",
        "3. Receives and records the results\n",
        "4. Continues reasoning to produce a final answer that incorporates the search results\n"
      ],
      "metadata": {
        "id": "YTxbWK5xcrpS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Imports\n",
        "import os\n",
        "from langchain.chat_models import init_chat_model\n",
        "from langchain_core.messages import HumanMessage\n",
        "from langchain.agents import initialize_agent, Tool, AgentType\n",
        "\n",
        "# 2) Define a simple calculator function\n",
        "def calculate_expression(expr: str) -> str:\n",
        "    \"\"\"\n",
        "    Safely evaluate a basic math expression and return the result.\n",
        "    Supports +, -, *, /, **, parentheses.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Only allow digits, operators, and whitespace\n",
        "        allowed = \"0123456789+-*/() .**\"\n",
        "        if any(c not in allowed for c in expr):\n",
        "            return \"Error: Invalid characters in expression.\"\n",
        "        result = eval(expr, {\"__builtins__\": {}}, {})\n",
        "        return str(result)\n",
        "    except Exception as e:\n",
        "        return f\"Error: {e}\"\n",
        "\n",
        "# 3) Wrap it as a LangChain Tool\n",
        "calc_tool = Tool(\n",
        "    name=\"calculator\",\n",
        "    func=calculate_expression,\n",
        "    description=\"Use this to compute math expressions, e.g. '23 * (7 + 5)**2'\",\n",
        ")\n",
        "\n",
        "# 4) Initialize your Mistral chat model\n",
        "model = init_chat_model(\n",
        "    \"mistral-large-latest\",\n",
        "    model_provider=\"mistralai\",\n",
        "    # ensure MISTRAL_API_KEY is set in your env\n",
        ")\n",
        "\n",
        "# 5) Build a ReAct-style agent with the calculator tool\n",
        "agent = initialize_agent(\n",
        "    tools=[calc_tool],\n",
        "    llm=model,\n",
        "    agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "# 6) Ask the agent a math question!\n",
        "response = agent.invoke({\"input\": \"What is 23 * (7 + 5)**2?\"})\n",
        "print(response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fgk6cPQwqxeX",
        "outputId": "474f3447-a549-45c3-b2a7-f51459b56cf0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-12-3116106650.py:38: LangChainDeprecationWarning: LangChain agents will continue to be supported, but it is recommended for new use cases to be built with LangGraph. LangGraph offers a more flexible and full-featured framework for building agents, including support for tool-calling, persistence of state, and human-in-the-loop workflows. For details, refer to the `LangGraph documentation <https://langchain-ai.github.io/langgraph/>`_ as well as guides for `Migrating from AgentExecutor <https://python.langchain.com/docs/how_to/migrate_agent/>`_ and LangGraph's `Pre-built ReAct agent <https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/>`_.\n",
            "  agent = initialize_agent(\n",
            "WARNING:langchain_mistralai.chat_models:Parameter `stop` not yet supported (https://docs.mistral.ai/api)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_mistralai.chat_models:Parameter `stop` not yet supported (https://docs.mistral.ai/api)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3mQuestion: What is 23 * (7 + 5)**2?\n",
            "Thought: I need to calculate the value of 23 * (7 + 5)**2.\n",
            "Action:\n",
            "```\n",
            "{\n",
            "  \"action\": \"calculator\",\n",
            "  \"action_input\": \"23 * (7 + 5)**2\"\n",
            "}\n",
            "```\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m3312\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mThe observation from the calculator tool is 3312.\n",
            "\n",
            "Final Answer: 3312\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "{'input': 'What is 23 * (7 + 5)**2?', 'output': '3312'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Reasoning**: The agent examines the user’s input and decides a calculation is needed.\n",
        "2. **Action**: It generates an action step like `Action: calculator(expression)`.\n",
        "3. **Execution**: LangChain calls `calculate_expression(expression)` under the hood.\n",
        "4. **Result**: The function returns a result string, which the agent incorporates into its next reasoning step and final answer."
      ],
      "metadata": {
        "id": "mevI8Ns9c25m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.prebuilt import create_react_agent\n",
        "\n",
        "# Create a higher-level executor\n",
        "agent_executor = create_react_agent(model, tools)\n",
        "\n",
        "# Simple chat\n",
        "out = agent_executor.invoke({\"messages\":[HumanMessage(content=\"Hi there!\")]})\n",
        "print(out[\"messages\"][-1].content)\n",
        "\n",
        "# Ask it to use the search tool\n",
        "out = agent_executor.invoke({\"messages\":[HumanMessage(content=\"Find me the latest news on AI agents.\")]})\n",
        "print(out[\"messages\"][-1].content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aaXoDKrfqyoa",
        "outputId": "954575ad-034b-468b-fa5c-a25feb260086"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello! How can I assist you today?\n",
            "[{'type': 'text', 'text': \"Here are some of the latest insights and news on AI agents:\\n\\n1. **IBM's Perspective on AI Agents in 2025**:\\n   - IBM experts, including Maryam Ashoori, Marina Danilevsky, Vyoma Gajjar, and Chris Hay, discuss the realistic expectations for AI agents in 2025. They aim to provide a balanced view beyond the hype, emphasizing that while there is excitement about the potential of AI agents, it's important to have a reasonable conversation about their capabilities and impacts. Industry reports suggest that 2025 could see a rise in the number and effectiveness of AI agents, with a focus on their autonomous capabilities and potential for profitability \"}, {'type': 'reference', 'reference_ids': [1]}, {'type': 'text', 'text': '.\\n\\n2. **How AI Agents Work in 2025**:\\n   - An article on Medium by Berto Mill highlights the transition of AI from simple task automation to complex, multi-step operations performed by sophisticated agentic systems. The implementation of these AI agents is described as straightforward, indicating a maturing understanding and integration of AI technologies into various applications '}, {'type': 'reference', 'reference_ids': [2]}, {'type': 'text', 'text': '.\\n\\n3. **Top AI Agents Transforming Business in 2025**:\\n   - Tredence provides an overview of the leading AI agents that are driving innovation in businesses. The article emphasizes that AI agents are now considered operational assets, expected to deliver real value rather than being mere experiments. A LangChain survey of over 1,300 professionals revealed that performance quality is the top concern for companies deploying AI agents, especially for small businesses where it matters more than cost or safety '}, {'type': 'reference', 'reference_ids': [3]}, {'type': 'text', 'text': '.'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for step in agent_executor.stream(\n",
        "    {\"messages\": [HumanMessage(content=\"What’s the weather in SF?\")]},\n",
        "    stream_mode=\"values\",\n",
        "):\n",
        "    # Each ‘step’ contains the newest message(s)\n",
        "    step[\"messages\"][-1].pretty_print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GyRHewcGq0Vy",
        "outputId": "d2507a58-5487-45c9-b0e0-a297ef66844b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "What’s the weather in SF?\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  tavily_search_results_json (9r4bZYX61)\n",
            " Call ID: 9r4bZYX61\n",
            "  Args:\n",
            "    query: weather in SF\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: tavily_search_results_json\n",
            "\n",
            "[{\"title\": \"San Francisco weather in July 2025 | Weather25.com\", \"url\": \"https://www.weather25.com/north-america/usa/california/san-francisco?page=month&month=July\", \"content\": \"| 27 Image 54: Mist 15°/13° | 28 Image 55: Fog 18°/12° | 29 Image 56: Overcast 18°/14° | 30 Image 57: Patchy rain possible 17°/14° | 31 Image 58: Partly cloudy 19°/15° |  |  | [...] Friday Jul 25 Image 8: Mist 0 mm 15°/14°Saturday Jul 26 Image 9: Overcast 0 mm 15°/14°Sunday Jul 27 Image 10: Mist 0 mm 15°/13°Monday Jul 28 Image 11: Fog 0 mm 18°/12°Tuesday Jul 29 Image 12: Overcast 0 mm 18°/14°Wednesday Jul 30 Image 13: Patchy rain possible 0 mm 17°/14°Thursday Jul 31 Image 14: Partly cloudy 0 mm 19°/15°Friday Aug 1 Image 15: Sunny 0 mm 20°/15°Saturday Aug 2 Image 16: Sunny 0 mm 18°/14°Sunday Aug 3 Image 17: Sunny 0 mm 18°/14°Monday Aug 4 Image 18: Sunny 0 mm 19°/14°Tuesday [...] | Sun | Mon | Tue | Wed | Thu | Fri | Sat |\\n| --- | --- | --- | --- | --- | --- | --- |\\n|  |  | 1 Image 28: Cloudy 20°/12° | 2 Image 29: Partly cloudy 20°/12° | 3 Image 30: Sunny 20°/12° | 4 Image 31: Sunny 20°/12° | 5 Image 32: Sunny 20°/13° |\\n| 6 Image 33: Sunny 19°/13° | 7 Image 34: Mist 19°/12° | 8 Image 35: Sunny 20°/12° | 9 Image 36: Partly cloudy 20°/13° | 10 Image 37: Partly cloudy 21°/13° | 11 Image 38: Sunny 22°/13° | 12 Image 39: Sunny 20°/13° |\", \"score\": 0.86358756}, {\"title\": \"Weather San Francisco in July 2025: Temperature & Climate\", \"url\": \"https://en.climate-data.org/north-america/united-states-of-america/california/san-francisco-385/t/july-7/\", \"content\": \"| Date | Weather | Max. | Min. | Rainrisk | Windspeed | Precipitation / Rainfall mm (in) | Humidity |\\n| --- | --- | --- | --- | --- | --- | --- | --- |\\n| 29. July | Scattered clouds | 16 °C 60.8 °F | 14 °C 57.2 °F | 20 % | 21 km/h 13 mph | 0mm 0 in | 83% |\\n| 30. July | Broken clouds | 16 °C 60.8 °F | 14 °C 57.2 °F | 20 % | 21 km/h 13 mph | 0mm 0 in | 86% |\\n| 31. July | Few clouds | 18 °C 64.4 °F | 15 °C 59 °F | 0 % | 13 km/h 8 mph | 0mm 0 in | 81% | [...] | 24. July | 17 °C | 62 °F | 22 °C | 72 °F | 13 °C | 55 °F | 14 °C | 57 °F | 0.0 mm | 0.0 inch. |\\n| 25. July | 17 °C | 62 °F | 22 °C | 72 °F | 13 °C | 55 °F | 14 °C | 57 °F | 0.0 mm | 0.0 inch. |\\n| 26. July | 16 °C | 61 °F | 22 °C | 72 °F | 13 °C | 55 °F | 14 °C | 57 °F | 0.0 mm | 0.0 inch. |\\n| 27. July | 16 °C | 61 °F | 22 °C | 72 °F | 13 °C | 55 °F | 14 °C | 57 °F | 0.1 mm | 0.0 inch. |\\n| 28. July | 16 °C | 61 °F | 22 °C | 71 °F | 13 °C | 55 °F | 14 °C | 57 °F | 0.1 mm | 0.0 inch. | [...] | 25. July | Broken clouds | 17 °C 62.6 °F | 13 °C 55.4 °F | 0 % | 15 km/h 9 mph | 0mm 0 in | 87% |\\n| 26. July | Overcast clouds | 16 °C 60.8 °F | 13 °C 55.4 °F | 0 % | 15 km/h 9 mph | 0mm 0 in | 88% |\\n| 27. July | Broken clouds | 17 °C 62.6 °F | 13 °C 55.4 °F | 0 % | 14 km/h 9 mph | 0mm 0 in | 86% |\\n| 28. July | Few clouds | 16 °C 60.8 °F | 14 °C 57.2 °F | 0 % | 19 km/h 12 mph | 0mm 0 in | 81% |\", \"score\": 0.8624794}, {\"title\": \"Weather in San Francisco in July 2025 - Detailed Forecast\", \"url\": \"https://www.easeweather.com/north-america/united-states/california/city-and-county-of-san-francisco/san-francisco/july\", \"content\": \"| Jul. 25 | Partly cloudy Partly cloudy | 21° /12° | 0 mm | 6 |  |\\n| Jul. 26 | Sunny Sunny | 20° /12° | 0 mm | 5 |  |\\n| Jul. 27 | Partly cloudy Partly cloudy | 21° /12° | 0.1 mm | 5 |  |\\n| Jul. 28 | Sunny Sunny | 20° /12° | 0.1 mm | 5 |  |\\n| Jul. 29 | Sunny Sunny | 21° /12° | 0 mm | 5 |  |\\n| Jul. 30 | Sunny Sunny | 21° /13° | 0.1 mm | 6 |  |\\n| Jul. 31 | Partly cloudy Partly cloudy | 20° /12° | 0 mm | 5 |  |\\n| Next | | | | | | [...] | 20 Partly cloudy 20° | 21 Partly cloudy 20° | 22 Sunny 22° | 23 Sunny 21° | 24 Sunny 22° | 25 Partly cloudy 21° | 26 Sunny 20° |\\n| 27 Partly cloudy 21° | 28 Sunny 20° | 29 Sunny 21° | 30 Sunny 21° | 31 Partly cloudy 20° |  |  |\", \"score\": 0.8390101}]\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "[{'type': 'text', 'text': 'The weather in San Francisco for the upcoming days in July 2025 is expected to be varied:\\n\\n- **July 28**: Foggy with temperatures ranging from 18°C to 12°C '}, {'type': 'reference', 'reference_ids': [0, 2]}, {'type': 'text', 'text': '.\\n- **July 29**: Overcast with temperatures between 18°C and 14°C '}, {'type': 'reference', 'reference_ids': [0, 2]}, {'type': 'text', 'text': '.\\n- **July 30**: Possible patchy rain with temperatures around 17°C to 14°C '}, {'type': 'reference', 'reference_ids': [0, 2]}, {'type': 'text', 'text': '.\\n- **July 31**: Partly cloudy with temperatures from 19°C to 15°C '}, {'type': 'reference', 'reference_ids': [0, 2]}, {'type': 'text', 'text': '.\\n\\nFor more detailed and up-to-date information, you can refer to sources like [Weather25.com](https://www.weather25.com/north-america/usa/california/san-francisco?page=month&month=July) and [Climate-Data.org](https://en.climate-data.org/north-america/united-states-of-america/california/san-francisco-385/t/july-7/).'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# a) Ask a math question\n",
        "resp = agent_executor.invoke({\"messages\": [HumanMessage(content=\"What is 23 * (7 + 5)**2?\")]})\n",
        "print(resp)"
      ],
      "metadata": {
        "id": "d0OaEVbNq3B5"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# b) Ask a search question\n",
        "resp = agent_executor.invoke({\"messages\": [HumanMessage(content=\"Find me the latest news on AI agents.\")]})\n",
        "print(resp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hcs8KAfXq4hp",
        "outputId": "f521f297-0568-4f25-a268-ad25f5456fcb"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'messages': [HumanMessage(content='Find me the latest news on AI agents.', additional_kwargs={}, response_metadata={}, id='987406ed-a424-4ba2-bb3e-75d097bdb5fe'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'bUSgxKB4j', 'function': {'name': 'tavily_search_results_json', 'arguments': '{\"query\": \"latest news on AI agents\"}'}, 'index': 0}]}, response_metadata={'token_usage': {'prompt_tokens': 116, 'total_tokens': 148, 'completion_tokens': 32}, 'model_name': 'mistral-large-latest', 'model': 'mistral-large-latest', 'finish_reason': 'tool_calls'}, id='run--e88cbde8-69a5-4deb-bed9-835e456b8a31-0', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'latest news on AI agents'}, 'id': 'bUSgxKB4j', 'type': 'tool_call'}], usage_metadata={'input_tokens': 116, 'output_tokens': 32, 'total_tokens': 148}), ToolMessage(content='[{\"title\": \"Daily AI Agent News - July 2025\", \"url\": \"https://aiagentstore.ai/ai-agent-news/2025-july\", \"content\": \"AI Agent Store Logo\\\\n\\\\n# Daily AI Agent News - July 2025\\\\n\\\\nBreaking News: Lindy Unveils No-Code AI Agent Platform Lindy’s new platform empowers developers to build multichannel AI agents for sales workflows without coding, while offering businesses cost-effective automation and newcomers a user-friendly entry point. With 7,000+ integrations and pre-built templates, this tool bridges technical complexity and practical implementation.\\\\n\\\\n---\\\\n\\\\n### For AI Agent Developers/Creators\\\\n\\\\nNew Tools & Frameworks [...] The AI agent landscape saw significant advancements on July 1, 2025, with Snowflake launching its Data Science Agent at Snowflake Summit 2025. This agentic AI tool uses Anthropic\\'s Claude LLMs to automate machine learning workflows—including data analysis and feature engineering—through natural language commands, reducing technical overhead for data scientists. Simultaneously, major banks accelerated adoption: BNY Mellon deployed two specialized AI agents—one automating system vulnerability [...] Today, Intuit\\'s launch of proactive AI agents within QuickBooks Online marks a pivotal development affecting all three audiences, automating workflows from invoicing to financial forecasting starting 07/01/2025. For developers, this rollout includes SDKs for embedded agents like the Payments Agent (predicting late invoices) and Accounting Agent (handling categorization), resolving integration hurdles via seamless human-AI collaboration. Business leaders gain quantifiable wins: Intuit reports 12\", \"score\": 0.8732259}, {\"title\": \"AI Agents in 2025: A Comprehensive Review and Future Outlook\", \"url\": \"https://medium.com/@sahin.samia/current-trends-in-ai-agents-use-cases-and-the-future-ahead-1026c4d753fd\", \"content\": \"# Latest Developments in AI Agents\\\\n\\\\nRecent research has yielded significant advancements in AI agent technology. Key areas of focus include:\\\\n\\\\n--\\\\n\\\\n--\\\\n\\\\n2\\\\n\\\\nSahin Ahmed, Data Scientist\\\\nSahin Ahmed, Data Scientist\\\\n\\\\n## Written by Sahin Ahmed, Data Scientist\\\\n\\\\nLifelong learner passionate about AI, LLMs, Machine Learning, Deep Learning, NLP, and Statistical Modeling to make a meaningful impact. MSc in Data Science.\\\\n\\\\n## Responses (2)\\\\n\\\\nHelp\\\\n\\\\nStatus\\\\n\\\\nAbout\\\\n\\\\nCareers\\\\n\\\\nPress\\\\n\\\\nBlog\\\\n\\\\nPrivacy\\\\n\\\\nRules\\\\n\\\\nTerms [...] The field of artificial intelligence (AI) is rapidly evolving, with AI agents emerging as a significant force driving the next wave of technological innovation. AI agents are autonomous systems capable of perceiving their environment, making decisions, and taking actions to achieve specific goals. This burgeoning field is experiencing rapid growth, with the AI agents market projected to soar from $5.1 billion in 2024 to $47.1 billion by 2030 . This article delves into the current trends in AI\", \"score\": 0.779339}, {\"title\": \"AI Agents in 2025: Expectations vs. Reality - IBM\", \"url\": \"https://www.ibm.com/think/insights/ai-agents-2025-expectations-vs-reality\", \"content\": \"We spoke with several IBM experts to cut through the hype, with the goal of holding a more reasonable conversation about AI agents and what they’re going to do. Our team of informed insiders includes:\\\\n\\\\nMaryam Ashoori, PhD: Director of Product Management, IBM® watsonx.ai™\\\\n\\\\nMarina Danilevsky: Senior Research Scientist, Language Technologies\\\\n\\\\nVyoma Gajjar: AI Technical Solutions Architect\\\\n\\\\nChris Hay: Distinguished Engineer\\\\n\\\\nIndustry newsletter\\\\n\\\\n### The latest tech news, backed by expert insights [...] “More and better agents” are on the way, predicts Time.1 “Autonomous ‘agents’ and profitability are likely to dominate the artificial intelligence agenda,” reports Reuters.2 “The age of agentic AI has arrived,” promises Forbes, in response to a claim from Nvidia’s Jensen Huang.3 [...] With that out of the way, what’s the conversation about agents over the coming year, and how much of it can we take seriously?\\\\n\\\\n### Decoding AI: Weekly News Roundup\\\\n\\\\nJoin our world-class panel of engineers, researchers, product leaders and more as they cut through the AI noise to bring you the latest in AI news and insights.\\\\n\\\\n## Narrative 1: 2025 is the year of the AI agent\", \"score\": 0.74363416}]', name='tavily_search_results_json', id='d907f079-b311-4681-ac20-fe52f0e76421', tool_call_id='bUSgxKB4j', artifact={'query': 'latest news on AI agents', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://aiagentstore.ai/ai-agent-news/2025-july', 'title': 'Daily AI Agent News - July 2025', 'content': \"AI Agent Store Logo\\n\\n# Daily AI Agent News - July 2025\\n\\nBreaking News: Lindy Unveils No-Code AI Agent Platform Lindy’s new platform empowers developers to build multichannel AI agents for sales workflows without coding, while offering businesses cost-effective automation and newcomers a user-friendly entry point. With 7,000+ integrations and pre-built templates, this tool bridges technical complexity and practical implementation.\\n\\n---\\n\\n### For AI Agent Developers/Creators\\n\\nNew Tools & Frameworks [...] The AI agent landscape saw significant advancements on July 1, 2025, with Snowflake launching its Data Science Agent at Snowflake Summit 2025. This agentic AI tool uses Anthropic's Claude LLMs to automate machine learning workflows—including data analysis and feature engineering—through natural language commands, reducing technical overhead for data scientists. Simultaneously, major banks accelerated adoption: BNY Mellon deployed two specialized AI agents—one automating system vulnerability [...] Today, Intuit's launch of proactive AI agents within QuickBooks Online marks a pivotal development affecting all three audiences, automating workflows from invoicing to financial forecasting starting 07/01/2025. For developers, this rollout includes SDKs for embedded agents like the Payments Agent (predicting late invoices) and Accounting Agent (handling categorization), resolving integration hurdles via seamless human-AI collaboration. Business leaders gain quantifiable wins: Intuit reports 12\", 'score': 0.8732259, 'raw_content': None}, {'url': 'https://medium.com/@sahin.samia/current-trends-in-ai-agents-use-cases-and-the-future-ahead-1026c4d753fd', 'title': 'AI Agents in 2025: A Comprehensive Review and Future Outlook', 'content': '# Latest Developments in AI Agents\\n\\nRecent research has yielded significant advancements in AI agent technology. Key areas of focus include:\\n\\n--\\n\\n--\\n\\n2\\n\\nSahin Ahmed, Data Scientist\\nSahin Ahmed, Data Scientist\\n\\n## Written by Sahin Ahmed, Data Scientist\\n\\nLifelong learner passionate about AI, LLMs, Machine Learning, Deep Learning, NLP, and Statistical Modeling to make a meaningful impact. MSc in Data Science.\\n\\n## Responses (2)\\n\\nHelp\\n\\nStatus\\n\\nAbout\\n\\nCareers\\n\\nPress\\n\\nBlog\\n\\nPrivacy\\n\\nRules\\n\\nTerms [...] The field of artificial intelligence (AI) is rapidly evolving, with AI agents emerging as a significant force driving the next wave of technological innovation. AI agents are autonomous systems capable of perceiving their environment, making decisions, and taking actions to achieve specific goals. This burgeoning field is experiencing rapid growth, with the AI agents market projected to soar from $5.1 billion in 2024 to $47.1 billion by 2030 . This article delves into the current trends in AI', 'score': 0.779339, 'raw_content': None}, {'url': 'https://www.ibm.com/think/insights/ai-agents-2025-expectations-vs-reality', 'title': 'AI Agents in 2025: Expectations vs. Reality - IBM', 'content': 'We spoke with several IBM experts to cut through the hype, with the goal of holding a more reasonable conversation about AI agents and what they’re going to do. Our team of informed insiders includes:\\n\\nMaryam Ashoori, PhD: Director of Product Management, IBM® watsonx.ai™\\n\\nMarina Danilevsky: Senior Research Scientist, Language Technologies\\n\\nVyoma Gajjar: AI Technical Solutions Architect\\n\\nChris Hay: Distinguished Engineer\\n\\nIndustry newsletter\\n\\n### The latest tech news, backed by expert insights [...] “More and better agents” are on the way, predicts Time.1 “Autonomous ‘agents’ and profitability are likely to dominate the artificial intelligence agenda,” reports Reuters.2 “The age of agentic AI has arrived,” promises Forbes, in response to a claim from Nvidia’s Jensen Huang.3 [...] With that out of the way, what’s the conversation about agents over the coming year, and how much of it can we take seriously?\\n\\n### Decoding AI: Weekly News Roundup\\n\\nJoin our world-class panel of engineers, researchers, product leaders and more as they cut through the AI noise to bring you the latest in AI news and insights.\\n\\n## Narrative 1: 2025 is the year of the AI agent', 'score': 0.74363416, 'raw_content': None}], 'response_time': 3.6}), AIMessage(content=[{'type': 'text', 'text': '### Latest News on AI Agents\\n\\n1. **Lindy Unveils No-Code AI Agent Platform**:\\n   Lindy has introduced a new platform that allows developers to create multichannel AI agents for sales workflows without any coding. This platform offers over 7,000 integrations and pre-built templates, making it easier for businesses to automate processes cost-effectively and providing a user-friendly entry point for newcomers '}, {'type': 'reference', 'reference_ids': [0]}, {'type': 'text', 'text': \".\\n\\n2. **Snowflake Launches Data Science Agent**:\\n   On July 1, 2025, Snowflake launched its Data Science Agent at the Snowflake Summit 2025. This tool uses Anthropic's Claude LLMs to automate machine learning workflows, including data analysis and feature engineering, through natural language commands. This reduces the technical overhead for data scientists \"}, {'type': 'reference', 'reference_ids': [0]}, {'type': 'text', 'text': '.\\n\\n3. **BNY Mellon Deploys Specialized AI Agents**:\\n   BNY Mellon has deployed two specialized AI agents, one of which automates system vulnerability assessments. This move highlights the accelerating adoption of AI agents in the banking sector '}, {'type': 'reference', 'reference_ids': [0]}, {'type': 'text', 'text': '.\\n\\n4. **Intuit Introduces Proactive AI Agents in QuickBooks Online**:\\n   Intuit has launched proactive AI agents within QuickBooks Online, automating workflows from invoicing to financial forecasting. This includes SDKs for embedded agents like the Payments Agent and Accounting Agent, enhancing human-AI collaboration and providing quantifiable benefits for businesses '}, {'type': 'reference', 'reference_ids': [0]}, {'type': 'text', 'text': '.\\n\\n5. **Market Growth and Future Outlook**:\\n   The AI agents market is projected to grow significantly, from $5.1 billion in 2024 to $47.1 billion by 2030. This rapid growth is driven by advancements in AI agent technology, which are autonomous systems capable of perceiving their environment, making decisions, and taking actions to achieve specific goals '}, {'type': 'reference', 'reference_ids': [1]}, {'type': 'text', 'text': '.\\n\\n6. **IBM Expert Insights on AI Agents**:\\n   IBM experts, including Maryam Ashoori and Marina Danilevsky, discuss the future of AI agents, predicting that \"more and better agents\" are on the way. They emphasize the need for a reasonable conversation about AI agents and their potential impact on various industries '}, {'type': 'reference', 'reference_ids': [2]}, {'type': 'text', 'text': '.\\n\\nFor more detailed information, you can visit the following sources:\\n- [AI Agent Store - Daily AI Agent News - July 2025](https://aiagentstore.ai/ai-agent-news/2025-july)\\n- [Medium - AI Agents in 2025: A Comprehensive Review and Future Outlook](https://medium.com/@sahin.samia/current-trends-in-ai-agents-use-cases-and-the-future-ahead-1026c4d753fd)\\n- [IBM - AI Agents in 2025: Expectations vs. Reality](https://www.ibm.com/think/insights/ai-agents-2025-expectations-vs-reality)'}], additional_kwargs={}, response_metadata={'token_usage': {'prompt_tokens': 1390, 'total_tokens': 2097, 'completion_tokens': 707}, 'model_name': 'mistral-large-latest', 'model': 'mistral-large-latest', 'finish_reason': 'stop'}, id='run--8d0aa667-eea7-4cee-acb0-ad7fc1abead9-0', usage_metadata={'input_tokens': 1390, 'output_tokens': 707, 'total_tokens': 2097})]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.prebuilt import create_react_agent"
      ],
      "metadata": {
        "id": "yhGOvOhpq6ML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent_executor = create_react_agent(\n",
        "    model,   # Your initialized ChatModel (e.g., Mistral)\n",
        "    tools    # The list of Tool objects (search_tool, calc_tool, etc.)\n",
        ")"
      ],
      "metadata": {
        "id": "_tMDRurWq8OX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "# Prepare a single user message\n",
        "messages = [HumanMessage(content=\"Hi there!\")]\n",
        "\n",
        "# Invoke synchronously\n",
        "out = agent_executor.invoke({\"messages\": messages})\n",
        "\n",
        "# The output dict contains a \"messages\" list\n",
        "# with your HumanMessage plus assistant replies\n",
        "print(out[\"messages\"][-1].content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tdDQiiRq9oe",
        "outputId": "ffe46ada-daa8-4a94-bf34-a355f8d6948f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langsmith.client:Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello! How can I assist you today?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "for step in agent_executor.stream({\"messages\": [HumanMessage(content=\"What's the weather in SF?\")]},\n",
        "                                 stream_mode=\"values\"):\n",
        "    # Each step contains the incremental reply\n",
        "    step[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        },
        "id": "0Nk32CdTq-6-",
        "outputId": "2db6d244-36ca-49af-dd4f-836897bdd233"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "What's the weather in SF?\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "HTTPStatusError",
          "evalue": "Error response 429 while fetching https://api.mistral.ai/v1/chat/completions: {\"message\":\"Requests rate limit exceeded\"}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPStatusError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-3295af0bc90a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessages\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHumanMessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m for step in agent_executor.stream({\"messages\": [HumanMessage(content=\"What's the weather in SF?\")]},\n\u001b[0m\u001b[1;32m      4\u001b[0m                                  stream_mode=\"values\"):\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Each step contains the incremental reply\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langgraph/pregel/__init__.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, subgraphs)\u001b[0m\n\u001b[1;32m   2459\u001b[0m                 \u001b[0;31m# with channel updates applied only at the transition between steps.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2460\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtick\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_channels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2461\u001b[0;31m                     for _ in runner.tick(\n\u001b[0m\u001b[1;32m   2462\u001b[0m                         \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2463\u001b[0m                         \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langgraph/prebuilt/chat_agent_executor.py\u001b[0m in \u001b[0;36mcall_model\u001b[0;34m(state, config)\u001b[0m\n\u001b[1;32m    743\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mStateSchema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRunnableConfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mStateSchema\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    744\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_model_input_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 745\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAIMessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_runnable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    746\u001b[0m         \u001b[0;31m# add agent name to the AIMessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3032\u001b[0m                         \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3033\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3034\u001b[0;31m                         \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3035\u001b[0m         \u001b[0;31m# finish the root run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3036\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   5414\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5415\u001b[0m     ) -> Output:\n\u001b[0;32m-> 5416\u001b[0;31m         return self.bound.invoke(\n\u001b[0m\u001b[1;32m   5417\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5418\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_merge_configs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m         return cast(\n\u001b[1;32m    369\u001b[0m             \u001b[0;34m\"ChatGeneration\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m             self.generate_prompt(\n\u001b[0m\u001b[1;32m    371\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m                 \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    945\u001b[0m     ) -> LLMResult:\n\u001b[1;32m    946\u001b[0m         \u001b[0mprompt_messages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_messages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverride\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    764\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m                 results.append(\n\u001b[0;32m--> 766\u001b[0;31m                     self._generate_with_cache(\n\u001b[0m\u001b[1;32m    767\u001b[0m                         \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m                         \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36m_generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1010\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_from_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1011\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1012\u001b[0;31m             result = self._generate(\n\u001b[0m\u001b[1;32m   1013\u001b[0m                 \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_mistralai/chat_models.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, messages, stop, run_manager, stream, **kwargs)\u001b[0m\n\u001b[1;32m    573\u001b[0m         \u001b[0mmessage_dicts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_message_dicts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m         \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m         response = self.completion_with_retry(\n\u001b[0m\u001b[1;32m    576\u001b[0m             \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessage_dicts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_mistralai/chat_models.py\u001b[0m in \u001b[0;36mcompletion_with_retry\u001b[0;34m(self, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 492\u001b[0;31m         \u001b[0mrtn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_completion_with_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mrtn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36mwrapped_f\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[0mcopy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0mwrapped_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatistics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatistics\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mretry_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mWrappedFn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m         \u001b[0mretry_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRetryCallState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_object\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mdo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDoAttempt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36miter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(rs)\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_post_retry_check_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretry_state\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"RetryCallState\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_explicit_retry\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretry_run_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_action_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutcome\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    447\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m                 \u001b[0;31m# Break a reference cycle with the exception in self._exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    478\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDoAttempt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 480\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    481\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# noqa: B902\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m                     \u001b[0mretry_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_mistralai/chat_models.py\u001b[0m in \u001b[0;36m_completion_with_retry\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m                 \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m                 \u001b[0m_raise_on_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_mistralai/chat_models.py\u001b[0m in \u001b[0;36m_raise_on_error\u001b[0;34m(response)\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhttpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0merror_message\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m         raise httpx.HTTPStatusError(\n\u001b[0m\u001b[1;32m    179\u001b[0m             \u001b[0;34mf\"Error response {response.status_code} \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;34mf\"while fetching {response.url}: {error_message}\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPStatusError\u001b[0m: Error response 429 while fetching https://api.mistral.ai/v1/chat/completions: {\"message\":\"Requests rate limit exceeded\"}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "# Prepare your input message\n",
        "streaming_input = {\"messages\": [HumanMessage(content=\"What's the weather in SF?\")]}\n",
        "\n",
        "# Iterate over streamed chunks\n",
        "for step in agent_executor.stream(\n",
        "    streaming_input,\n",
        "    stream_mode=\"values\"\n",
        "):\n",
        "    # 'step' is a dict with a \"messages\" list\n",
        "    # The last message is the assistant's partial reply\n",
        "    step[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZeJE6vorAYA",
        "outputId": "18361e58-ecd7-4409-dfa3-0723692d9541"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "What's the weather in SF?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langsmith.client:Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  tavily_search_results_json (n1sYldF9J)\n",
            " Call ID: n1sYldF9J\n",
            "  Args:\n",
            "    query: weather in SF\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langsmith.client:Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: tavily_search_results_json\n",
            "\n",
            "[{\"title\": \"Weather in San Francisco in May 2025 (California)\", \"url\": \"https://world-weather.info/forecast/usa/san_francisco/may-2025/\", \"content\": \"Weather in San Francisco in May 2025. San Francisco Weather Forecast for May ... Sunday, 11 May. +52°. Day. +63°. Clear sky. Monday, 12 May. +54°. Day. +63°. Few\", \"score\": 0.94401723}, {\"title\": \"Weather in San Francisco in May 2025 - Detailed Forecast\", \"url\": \"https://www.easeweather.com/north-america/united-states/california/city-and-county-of-san-francisco/san-francisco/may\", \"content\": \"| 66° /48° | 0 in | 5 |  |\\n| May 11 | \\nSunny\\n| 66° /48° | 0 in | 5 |  |\\n| May 12 | \\nSunny\\n| 64° /48° | 0 in | 5 |  |\\n| May 13 | \\nSunny\\n| 68° /50° | 0 in | 6 |  |\\n| May 14 | \\nSunny\\n| 66° /50° | 0 in | 6 |  |\\n| May 15 | \\nOvercast\\n| 64° /50° | 0 in | 5 |  |\\n| May 16 | \\nCloudy\\n| 64° /50° | 0 in | 5 |  |\\n| May 17 | \\nOvercast\\n| 66° /50° | 0 in | 5 |  |\\n| May 18 | \\nSunny\\n| 68° /48° | 0 in | 6 |  |\\n| May 19 | \\nSunny\\n| 64° /50° | 0 in | 5 |  |\\n| May 20 | \\nSunny\\n| 66° /50° | 0 in | 5 |  |\\n| May 21 | [...] More\\n\\nNew! Chat with our AI weatherman - it’s amazing and free.\\n\\n\\n28\\nDry days\\n\\n3\\nRainy days\\n\\n0\\nSnow days\\n\\n64.4°/50°\\nTemperatures\\n\\n12.4 mph\\nAvg max wind\\n\\n72 %\\nAvg humidity\", \"score\": 0.94301796}, {\"title\": \"Weather in San Francisco\", \"url\": \"https://www.weatherapi.com/\", \"content\": \"{'location': {'name': 'San Francisco', 'region': 'California', 'country': 'United States of America', 'lat': 37.775, 'lon': -122.4183, 'tz_id': 'America/Los_Angeles', 'localtime_epoch': 1746989366, 'localtime': '2025-05-11 11:49'}, 'current': {'last_updated_epoch': 1746989100, 'last_updated': '2025-05-11 11:45', 'temp_c': 14.4, 'temp_f': 57.9, 'is_day': 1, 'condition': {'text': 'Partly cloudy', 'icon': '//cdn.weatherapi.com/weather/64x64/day/116.png', 'code': 1003}, 'wind_mph': 13.2, 'wind_kph': 21.2, 'wind_degree': 263, 'wind_dir': 'W', 'pressure_mb': 1017.0, 'pressure_in': 30.04, 'precip_mm': 0.0, 'precip_in': 0.0, 'humidity': 70, 'cloud': 75, 'feelslike_c': 12.8, 'feelslike_f': 55.1, 'windchill_c': 9.1, 'windchill_f': 48.4, 'heatindex_c': 11.0, 'heatindex_f': 51.7, 'dewpoint_c': 10.1, 'dewpoint_f': 50.2, 'vis_km': 16.0, 'vis_miles': 9.0, 'uv': 6.2, 'gust_mph': 15.6, 'gust_kph': 25.2}}\", \"score\": 0.9415873}]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langsmith.client:Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G7BZLvBbri_2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}